---
title: 
- "Analysis Experiment 6: Is convergence less trustworthy when informants are biased?"
author: 
- "Hugo Mercier, Jan Pf√§nder" 
date: "2023-02-02"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = TRUE) 
```

```{r, message=FALSE}
library(tidyverse) # for everything
library(tidyr) # some additional, neat tidy reshaping functions
library(lme4) # for linear mixed models
library(lmerTest) # p-values for mixed models
library(broom) # for tidy outputs of regressions
library(broom.mixed) # for tidy outputs of linear mixed models
library(ggridges) # for plots
library(gghalves) # for plots
library(ggbeeswarm) # Special distribution-shaped point jittering
library(scales)  # for nice axis labels
library(modelsummary) # regression tables
library(patchwork) # combine plots
library(ggrepel) # fit labels in plots 
```

### Some themes and functions

```{r, echo=FALSE}
# round numbers from models
rounded_numbers <- function(x) mutate_if(x, is.numeric, round, 3)

#set general theme for plots
plot_theme <- theme_minimal(base_size = 12) +
  theme(# Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.7)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.3), color = "grey70"),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold axis titles
        axis.title = element_text(face = "bold"))
```

### Read data
```{r, message=FALSE}
# read data
d <- read_csv("./data/cleaned.csv") %>% 
  # make number_options a factor
  mutate(number_options = relevel(as.factor(number_options), ref = "three"))
```

### H1a: In the three options condition, participants perceive an estimate of an independent informant as more accurate the more it converges with the estimates of other informants.

To test this hypothesis, we only consider participants assigned to the `3` options condition.

We use a linear mixed effect model with random intercept and random slope per participant. Should this model yield convergence issues, we will use a model with random intercept only.

In all our models we treat `convergence` as a continuous variable. We will, however, include robustness checks where we treat convergence as a categorical variable, allowing to inspect difference between different levels.

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy_3options <- lmer(accuracy ~ convergence + (1 + convergence | id), 
                       data = d %>% filter(number_options == "three"))

```


### H1b:  In the three options condition, participants perceive an independent informant as more competent the more their estimate converges with the estimates of other informants.

We will proceed in the same way for `competence` as we did for `accuracy` above.

```{r, warning=FALSE, message=FALSE}
# models for competence

# random intercept and slope by participants
model_competence_3options <- lmer(competence ~ convergence + 
                           (1 + convergence | id), 
                         data = d %>% filter(number_options == "three"))
```

### H2a: The effect of convergence on accuracy (H1a) is more positive in a context when informants can choose among ten response options compared to when they can choose among only three. 

To test this hypothesis, we consider the full data.

The resulting estimate of the interaction term will provide the test for our hypothesis. 

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy <- lmer(accuracy ~ convergence + number_options + 
                            number_options*convergence + (1 + convergence | id), 
                       data = d)
```

  
### H2b: The effect of convergence on competence (H1b) is more positive in a context when informants can choose among ten response options compared to when they can choose among only three. 

To test this hypothesis, we consider the full data.

The resulting estimate of the interaction term will provide the test for our hypothesis. 

```{r, warning=FALSE, message=FALSE}
# models for competence

# random intercept and slope by participants
model_competence <- lmer(competence ~ convergence + number_options + 
                            number_options*convergence + (1 + convergence | id), 
                       data = d)
```

Show all results
```{r, echo=FALSE}
models <- list("Accuracy (3 options condition)" = model_accuracy_3options, 
               "Competence (3 options condition)" = model_competence_3options, 
               "Accuracy" = model_accuracy, 
               "Competence" = model_competence)
modelsummary::modelsummary(models, stars = TRUE)
```

## Research question 

### RQ1: Within the 10 choice options condition, is the effect of convergence more positive for the set of stimuli with greater distance?

```{r, warning=FALSE, message=FALSE}
# models for accuracy

# random intercept and slope by participants
model_accuracy_10options <- lmer(accuracy ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 + convergence | id), 
                       data = d)


# models for competence

# random intercept and slope by participants
model_competence_10options <- lmer(competence ~ convergence + stimuli_10_version + 
                            stimuli_10_version*convergence + (1 + convergence | id), 
                       data = d)

```

```{r}
models <- list("Accuracy (10 options conditions)" = model_accuracy_10options, 
               "Competence (10 options conditions)" = model_competence_10options)
modelsummary::modelsummary(models, stars = TRUE)
```



### Plots

```{r, echo=FALSE}
# make a categorical variable from `convergence`
d <- d %>% 
  mutate(convergence_categorical = recode_factor(convergence, 
                                                 `0` = "opposing majority", 
                                                 `1` = "divergence", 
                                                 `2` = "majority", 
                                                 `3` = "consensus",
                                                 .default = NA_character_)
         )

levels(d$convergence_categorical)
```

#### Three options only

```{r, echo=FALSE}
# plot for accuracy
plot_accuracy <- ggplot(d %>% filter(number_options == "three"),
       aes(x = convergence_categorical, y = accuracy, fill = convergence_categorical)) +
  geom_half_violin(position = position_nudge(x = -.2),
                   adjust=2, alpha = .8,
                   side = "l") +
  stat_summary(fun = "mean", geom = "point", size = 1, shape = 21) +
  stat_summary(fun = "mean", geom = "line", size = 1, linetype = "dashed") +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .2) +
  # Add nice labels
  labs(x = "Convergence", y = "Accuracy") +
  scale_fill_viridis_d(option = "plasma", begin = 0.1) +
  guides(fill = FALSE) +
  plot_theme + 
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

```{r, echo=FALSE}
# plot for competence
plot_competence <- ggplot(d %>% filter(number_options == "three"),
       aes(x = convergence_categorical, y = competence, fill = convergence_categorical)) +
  geom_half_violin(position = position_nudge(x = -.2),
                   adjust=2, alpha = .8,
                   side = "l") +
  stat_summary(fun = "mean", geom = "point", size = 1, shape = 21) +
  stat_summary(fun = "mean", geom = "line", size = 1, linetype = "dashed") +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .2) +
  # Add nice labels
  labs(x = "Convergence", y = "Competence") +
  scale_fill_viridis_d(option = "plasma") +
  guides(fill = FALSE) +
  plot_theme + 
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

```{r, echo=FALSE}
plot_accuracy + plot_competence
```

#### Interaction

```{r, echo=FALSE}
# plot for accuracy
plot_accuracy <- ggplot(d,
       aes(x = convergence_categorical, y = accuracy, fill = number_options)) +
  geom_half_violin(position = position_nudge(x = -.2),
                   adjust=2, alpha = .4,
                   side = "l") +
  stat_summary(fun = "mean", geom = "point", size = 2, shape = 21) +
  stat_summary(fun = "mean", geom = "line", size = 1, linetype = "dashed") +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .2) +
  # Add nice labels
  labs(x = "Convergence", y = "Accuracy", fill = NULL) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9")) +
  plot_theme + 
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

```{r, echo=FALSE}
# plot for competence
plot_competence <- ggplot(d,
       aes(x = convergence_categorical, y = competence, fill = number_options)) +
  geom_half_violin(position = position_nudge(x = -.2),
                   adjust=2, alpha = .4,
                   side = "l") +
  stat_summary(fun = "mean", geom = "point", size = 2, shape = 21) +
  stat_summary(fun = "mean", geom = "line", size = 1, linetype = "dashed") +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = .2) +
  # Add nice labels
  labs(x = "Convergence", y = "Competence", fill = NULL) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9")) +
  plot_theme + 
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

```{r main-plot, echo=FALSE}
ggpubr::ggarrange(plot_accuracy, plot_competence, common.legend = TRUE)
```

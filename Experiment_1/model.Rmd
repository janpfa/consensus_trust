---
title: "Model for numerical choice scenarios (experiments 1 to 3)"
output: 
  html_document: 
    keep_md: yes
date: "2023-04-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo=FALSE}
# Load packages
library(tidyverse)
library(gghalves) # for plots
library(patchwork)
library(lme4) # for linear mixed models
library(lmerTest) # p-values for mixed models
library(broom) # for tidy outputs of regressions
library(broom.mixed) # for tidy outputs of linear mixed models
```

```{r, echo = FALSE}
#set general theme for plots
plot_theme <- theme_minimal(base_size = 12) +
  theme(# Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.7)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.3), color = "grey70"),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold axis titles
        axis.title = element_text(face = "bold"))
```

```{r}
# ensure this script returns the same results on each run
set.seed(8726)
```

## Explaining the model

```{r}
# Imagine a situation in which answers range from 1000 to 2000
range <- 1000
# and a population of n = 999
population <- 999
# and we observe samples of 3
sample <- 3
```


### Define competence and its distribution 

We define competence as the value of the standard deviation from which one's answer is drawn. We suppose that answers for all individuals are drawn from normal distributions. Each individual has their own normal distribution. All normal distributions are centered around the true answer, but they differ in their standard deviations. The higher the competence, the lower the standard deviation, i.e. the more certain a guess drawn from the normal distribution will be close to the true answer. 

We (arbitrarily) set the *lowest* competence to the range of possible values, in our case 2000 - 1000 = 1000. We set the *highest* competence to 1/1000 of the range of possible values, in our case 1. 

```{r}
# Define the x-axis values
x <- seq(1000, 2000, length.out = 1000)

# Define the PDFs for the two distributions
high_competence_pdf <- dnorm(x, mean = 1500, sd = 1)
low_competence_pdf <- dnorm(x, mean = 1500, sd = 1000)

# Create the plot
ggplot() +
  geom_line(aes(x, high_competence_pdf, color = "Highest Competence Individual \n (SD = 1, Mean = 1500)"), size = 1) +
  geom_line(aes(x, low_competence_pdf, color = "Lowest Competence Individual \n (SD = 1000, Mean = 1500)"), size = 1) +
  labs(x = "Competence Level", y = "Density", color = "Data generating function for") +
  ggtitle("Competence Level Distributions") +
  scale_color_manual(values = c("Highest Competence Individual \n (SD = 1, Mean = 1500)" = "blue", 
                                "Lowest Competence Individual \n (SD = 1000, Mean = 1500)" = "red")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

We suppose that these competence levels are drawn from a random uniform distribution. That is, in a population, each level of competence is equally likely to occur. 

In order to be able to compare competence measures across different ranges, we want to scale it to reach from 0 (minimal competence) to 1 (maximal competence). 

```{r}
# 1: Distribution of competence
data <- data.frame(competence = runif(population, 
                                      min = range/1000, 
                                      max = range)) %>% 
  # Scale competence to reach from 0 to 1 (instead of 1 to 1000).
  # Reverse the scale such that 1 becomes maximal competence (instead of 0).
  mutate(competence_scaled = 1 - ((competence - 1) / (1000-1)))


# generated population
ggplot(data, aes(x = competence_scaled)) +
  geom_histogram() + 
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = 0.1)) + 
  annotate("text", x = 0.9, y = 100, label = paste0("n = ", population), color = "red") +
  labs(title = "(sampled) Population of competence drawn from \n uniform population distribution", 
       y = "Count")
```
### Generate a choice for each individual based on their competence. 

The numbers that we generate from the normal distributions are tuncated such that they all lie within the pre-defined range (1000 to 2000)

```{r}
# 2: Draw individual answers based on competence levels 
mean <- 1500

data$answer <- data$competence %>% 
  purrr::map_dbl(function(x){ 
    
    sd <- x
    
    answer = truncnorm::rtruncnorm(1, mean = mean, sd = sd, a = 1000, b = 2000)
  }
  )

ggplot(data, aes(x = answer)) +
  geom_histogram()
```

### Measure accuracy

We measure accuracy as the (absolute value of) distance between the chosen answer and the true answer for each individual.

We let this distance be negative, such that higher values represent higher accuracy. 

```{r}
# 3: measure accuracy
data <- data %>% 
  mutate(accuracy = -1 * abs(mean - answer))

ggplot(data, aes(x = accuracy)) +
  geom_histogram() +

ggplot(data, aes(x = competence_scaled, y = accuracy)) +
  geom_point()
```

### Draw samples, calculate convergence & compare average outcomes

### Randomly draw samples from the population

```{r}
# 4: randomly assign samples in population
data <- data %>% mutate(sample_id = rep(1:(population/sample), sample))
```

### Calculate convergence 

We define convergence as the standard deviation of the answers within a sample (multiplied by -1 so that higher values correspond to greater convegence)

```{r}
# 5: calculate convergence 
data <- data %>% 
  # identify how often a one type of answer occurs in one group
  group_by(sample_id) %>% 
  mutate(convergence = -1 * sd(answer)) %>% 
  ungroup()

ggplot(data, aes(x = convergence)) +
  geom_histogram()
```

### Compute average accuracy/competence per sample.

```{r}
# 6: calculate accuracy and competence per sample

# compute the summary statistics for each sample
results <- data %>% 
  group_by(sample_id, convergence) %>% 
  summarize(competence_mean = mean(competence_scaled),
            accuracy_mean = mean(accuracy)) %>% 
  # store simulation information
  mutate(population = population, 
         sample = sample)
```

```{r}
accuracy_plot <- ggplot(results, aes(x = convergence, y = accuracy_mean)) +
  geom_point() +
  # Add nice labels
    labs(x = "Convergence \n(SDs within samples of three)", 
         y = "Accuracy \n(average distance to true mean)") +
  plot_theme 

competence_plot <- ggplot(results, aes(x = convergence, y = competence_mean)) +
  geom_point() +
  # Add nice labels
    labs(x = "Convergence \n(SDs within samples of three)", 
         y = "Competence \n(SD for data generating function, transposed)") +
  plot_theme 

accuracy_plot + competence_plot
```

## Functions

### Data generating functions

#### Single population

```{r, echo=FALSE}
simulate_single_population <- function(population, sample, min_range, max_range, 
                                       competence_ratio = 1000) {
  
  # Check if population is not divisible by sample without remainder
  
  if (population %% sample != 0) {
    warning("Population is not divisible by sample without remainder.")
  }
  
  # if necessary, alter population size so that it is divisible by sample without rest
  n_possible_samples <- floor(population/sample)
  
  # Biggest possible population to pick
  possible_population = n_possible_samples * sample
  
  # Issue warning if different population size used
  if (population %% sample != 0) {
    warning(paste0("Chosen population size is ", possible_population))
  }
  # change population value (if divisible without remainder, it'll be the same)
  population = possible_population
  
  # Set variables
  range = max_range - min_range
  mean = min_range + (max_range - min_range)/2
  
  # 1: Distribution of competence
  data <- data.frame(competence = runif(population, 
                                        min = range/competence_ratio, 
                                        max = range)) %>% 
    # Scale competence to reach from 0 to 1.
    # Reverse the scale such that 1 becomes maximal competence (instead of 0).
    mutate(competence_scaled = 1 - ((competence - range/competence_ratio) / 
                                      (range - range/competence_ratio)))
  
  # 2: Draw individual answers based on competence levels 
  
  data$answer <- data$competence %>% 
    purrr::map_dbl(function(x){ 
      
      sd <- x
      
      answer = truncnorm::rtruncnorm(1, mean = mean, sd = sd, a = min_range, 
                                     b = max_range)
    }
    )
  
  # 3: measure accuracy
  data <- data %>% 
    mutate(accuracy = -1 * abs(mean - answer))
  
  # 4: randomly assign samples in population
  data <- data %>% mutate(sample_id = rep(1:(population/sample), sample))
  
  # 5: calculate convergence 
  data <- data %>% 
    group_by(sample_id) %>% 
    mutate(convergence = -1 * sd(answer)) %>% 
    ungroup()
  
  # 6: calculate accuracy and competence levels per sample
  results <- data %>% 
    group_by(sample_id, convergence) %>% 
    summarize(competence_mean = mean(competence_scaled),
              accuracy_mean = mean(accuracy)) %>% 
    # store simulation information
    mutate(population = population, 
           sample = sample) %>% 
    ungroup()
  
  return(results)
}

```

```{r, echo=FALSE}
# # # test output (set return to data above)
# test <- simulate_single_population(population = 500, sample = 3,
#                                    min_range = 1, max_range = 6,
#                                    competence_ratio = 1000)
```

#### Various populations

```{r, echo=FALSE}
simulate_various_populations <- function(iterations,...) {
  
  # create data frame with model results for generated samples
  various_populations <- 1:iterations %>% 
    purrr::map_df(function(x){
      # this is essentially a for loop - do the following for each 
      # element in 1:iterations
      
      results <- simulate_single_population(...)
      
      # identify iteration
      results$iteration <- x
      
      # To keep track of progress
      if (x %% 50 == 0) {print(paste("iteration number ", x))}
      
      return(results)
      
    }) %>% 
    # store simulation information
    mutate(total_iterations = iterations)
  
  return(various_populations)
}
```


```{r, echo=FALSE}
# # try out code
# test <- simulate_various_populations(population = 500, sample = 3,
#                                    min_range = 1, max_range = 6,
#                                    competence_ratio = 1000,
#                                    iterations = 100)
```


#### Vary sample size

This function allows us to investigate how accuracy and competence change with varying the sample size. 

The simulations that this function executes will take quite some time. Therefore, we do not want to run it every time we render this document. Instead we want to store the output of the power simulation in a `.csv` file, and have an integrated "stop" mechanism to prevent execution when that file already exists. To achieve this, we make `file_name` a mandatory argument. If a file with that name already exists, the function will not be executed.

```{r, echo=FALSE}
# function for different sample sizes
vary_sample <- function(file_name, n, ...) {
  
  # only run analysis if a file with that name does not yet exists
  if (!file.exists(paste0("data/", file_name))) {
    
    # do the `simulate_various_populations()` function for each sample size 
    # and store the results in a new data_frame
    data <- n %>% purrr::map_df(function(n_sample){
      # this is essentially a for loop - 
      # do the following for each 
      # element data$n_subj
      
      # To keep track of progress
      print(paste("tested sample number = ", n_sample))
      
      # run power calculation
      result <- simulate_various_populations(
        sample = n_sample, ...)
      
      return(result)
      
    })
    
    write_csv(data, paste0("data/", file_name))
  }
}
```

```{r, echo=FALSE}
# # try out code
# n <- c(3, 10)
# test <- vary_sample(population = 999,
#                     min_range = 1000, max_range = 2000,
#                     competence_ratio = 1000,
#                     iterations = 100,
#                     file_name = "test.csv",
#                     n = n)
```

### Plot functions

#### a) Plot various populations

First, we want a function that plots results obtained by the `simulate_various_populations` function. 
```{r, echo=FALSE}
plot_results <- function(data, outcome = "everything") {
  
  d <- data
  
  # extract simulation info
  simulation_info <- d %>% 
    summarize(across(c(population, sample, total_iterations), max)) %>% 
    pivot_longer(everything(), names_to = "parameter", values_to = "value") %>% 
    gridExtra::tableGrob(cols = NULL, rows = NULL)
  
  
  plot_accuracy <- ggplot(d, aes(x = convergence, y = accuracy_mean)) +
    geom_hex() +
    # Add nice labels
    labs(x = "Convergence \n(SDs within samples of three)", 
         y = "Accuracy \n(average distance to true mean)") +
    plot_theme +
  scale_fill_viridis_c(option = "plasma", begin = 0.1) 
  
  plot_competence <- ggplot(d, aes(x = convergence, y = competence_mean)) +
    geom_hex() +
    # Add nice labels
    labs(x = "Convergence \n(SDs within samples of three)", 
         y = "Competence \n(SD for data generating function, transposed)") +
    plot_theme +
  scale_fill_viridis_c(option = "plasma", begin = 0.1) 
  
  if (outcome == "everything") {
    
    return((plot_accuracy | plot_competence) / simulation_info)
  }
  
  if (outcome == "accuracy") {
    
    return(plot_accuracy)
  }
  
  if (outcome == "competence") {
    
    return(plot_competence)
  }
  
}
```


#### b) Plot varying sample & options

Second, we want a function that plots results obtained by the `vary_sample()` function. 

```{r, echo=FALSE}
plot_results_vary <- function(data, outcome = "everything", sample) {
  
  # get different samples as vector
  samples <- data %>% 
    reframe(unique(as.character(sample))) %>% pull()
  
  
  # empty list
  plot_list <- list()
  
  # make plots fro each moderator
  for (i in samples) {
    
    plot_data <- data %>%
      filter(sample == i)
    
    sample_plot <- plot_results(data = plot_data)
    
    plot_list[[i]] <- sample_plot
  }
  
  return(plot_list)
}
```

## Simulation

### Generate data

```{r, message=FALSE}
n <- c(3, 10, 20, 50)

# run simulation and store results in .csv files
vary_sample(population = 999,
            min_range = 1000, max_range = 2000,
            competence_ratio = 1000,
            iterations = 100,
            file_name = "model_vary_3_10_20_50.csv",
            n = n)

# read simulated data from .csv files
data <- read_csv("data/model_vary_3_10_20_50.csv")
```

### Plot

```{r}
# plot results
plot_results_vary(data)
```

### Analyze & Compare to participant data

It's not clear to me yet what we would want to compare. 

In the experiment, we generated guesses from two different distribution widths: one with SD = 20 and another with SD = 150. In the model, this corresponds to the competence. However, we did not measure the observed SD of the guesses (which is the measure of convergence I rely on in the models). 

Below, what I did was predict accuracy and competence for convergence levels of 20 and 150 based on a regression run on the model data. We could compare that to the participant data - but it's not quite the same thing...

```{r, echo=FALSE}
# possible analysis
regression_data <- data %>% 
  filter(sample %in% c(3, 10)) %>% 
  mutate(
    # makes sample a factor
    sample = as.factor(sample),
    # Scale accuracy to reach from 1 to 7 
    # a) scale the variable to range between 0 and 1
    scaled_accuracy_mean = (accuracy_mean - min(accuracy_mean)) / 
      (max(accuracy_mean) - min(accuracy_mean)),
    # b) scale to reach from 1 to 7
    accuracy_mean = 6*scaled_accuracy_mean + 1,
    # scale competence to reach from 1 to 7 (instead of 0 to 1)
    competence_mean = 6*competence_mean + 1
  ) 
  
accuracy <- lm(accuracy_mean ~ convergence + sample + convergence*sample, 
               data = regression_data) 
competence <- lm(competence_mean ~ convergence + sample + convergence*sample, 
               data = regression_data) 

summary(competence)

# Create a new data frame with values for prediction
values_to_predict <- data.frame(convergence = c(-20, -150), 
                                sample = rep(c("3", "10"), each = 2))

# Predict values using the fitted model
predictions <- bind_cols(values_to_predict,
                         predict(accuracy, newdata = values_to_predict) %>% 
                           tidy() %>% 
                           pivot_longer(x, 
                                       names_to = "bla", 
                                       values_to = "predicted_accuracy") %>% 
                           select(predicted_accuracy),
                         predict(competence, newdata = values_to_predict) %>% 
                           tidy() %>% 
                           pivot_longer(x, 
                                       names_to = "bla", 
                                       values_to = "predicted_competence") %>% 
                           select(predicted_competence)
                         )

predictions

```




